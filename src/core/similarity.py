#!/usr/bin/env python3
"""
Obsidian Knowledge Assistant - Similarity Analyzer
å†…å®¹ç›¸ä¼¼åº¦åˆ†ææ¨¡å—
"""

import os
import re
import math
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Set
from dataclasses import dataclass
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.analyzer import Note


@dataclass
class SimilarityResult:
    """ç›¸ä¼¼åº¦ç»“æœ"""
    note1: str
    note2: str
    similarity: float
    common_words: List[str]
    reason: str  # ç›¸ä¼¼åŸå› ï¼š'content', 'title', 'tags'
    
    def __repr__(self):
        return f"SimilarityResult({self.note1} <-> {self.note2}: {self.similarity:.2%})"


class SimilarityAnalyzer:
    """ç›¸ä¼¼åº¦åˆ†æå™¨"""
    
    def __init__(self, notes: Dict[str, Note]):
        self.notes = notes
        
        # åœç”¨è¯ï¼ˆä¸­è‹±æ–‡å¸¸è§è¯ï¼‰
        self.stopwords = set([
            # è‹±æ–‡åœç”¨è¯
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'should', 'could', 'may', 'might', 'must', 'can', 'this',
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'my', 'your', 'his', 'her', 'its', 'our', 'their', 'me', 'him', 'us',
            'them', 'what', 'which', 'who', 'when', 'where', 'why', 'how',
            # ä¸­æ–‡åœç”¨è¯
            'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'ä¸ª', 'ä¸', 'äºº', 'è¿™', 'ä¸­',
            'å¤§', 'ä¸º', 'ä¸Š', 'æ¥', 'ä»–', 'æ—¶', 'è¦', 'å°±', 'å‡º', 'ä»¬', 'åˆ°', 'è¯´',
            'ä¹Ÿ', 'åœ°', 'å¥¹', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡', 'çœ‹', 'å¥½', 'è‡ª', 'è€Œ', 'èƒ½',
            'ä¸‹', 'å¯¹', 'äº', 'æŠŠ', 'é‚£', 'ä¸', 'å»', 'å¾—', 'èµ·', 'è¿˜', 'ä»', 'ç”¨'
        ])
        
        # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        self.min_similarity = float(os.getenv('SIMILARITY_MIN_THRESHOLD', '0.3'))
        
        # è¯å‘é‡ç¼“å­˜
        self._word_vectors = {}
        self._idf_scores = {}
    
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
        # ç§»é™¤é“¾æ¥
        text = re.sub(r'\[\[.*?\]\]', '', text)
        text = re.sub(r'\[.*?\]\(.*?\)', '', text)
        # ç§»é™¤ Markdown æ ‡è®°
        text = re.sub(r'[#*_`]', '', text)
        
        # åˆ†å‰²ä¸ºå•è¯ï¼ˆä¿ç•™ä¸­è‹±æ–‡ï¼‰
        # è‹±æ–‡å•è¯
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        # ä¸­æ–‡å­—ç¬¦ï¼ˆæŒ‰å­—åˆ†ï¼‰
        chinese = re.findall(r'[\u4e00-\u9fff]+', text)
        for chars in chinese:
            words.extend(list(chars))
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        words = [w for w in words if w not in self.stopwords and len(w) > 1]
        
        return words
    
    def _calculate_idf(self):
        """è®¡ç®— IDF (Inverse Document Frequency)"""
        # ç»Ÿè®¡æ¯ä¸ªè¯å‡ºç°åœ¨å¤šå°‘ä¸ªæ–‡æ¡£ä¸­
        doc_count = defaultdict(int)
        total_docs = len(self.notes)
        
        for note in self.notes.values():
            words = set(self._tokenize(note.content))
            for word in words:
                doc_count[word] += 1
        
        # è®¡ç®— IDF
        for word, count in doc_count.items():
            self._idf_scores[word] = math.log(total_docs / count)
    
    def _get_tfidf_vector(self, note: Note) -> Dict[str, float]:
        """è·å–ç¬”è®°çš„ TF-IDF å‘é‡"""
        if note.name in self._word_vectors:
            return self._word_vectors[note.name]
        
        words = self._tokenize(note.content)
        
        # è®¡ç®— TF (Term Frequency)
        word_count = Counter(words)
        total_words = len(words)
        
        if total_words == 0:
            self._word_vectors[note.name] = {}
            return {}
        
        # è®¡ç®— TF-IDF
        tfidf = {}
        for word, count in word_count.items():
            tf = count / total_words
            idf = self._idf_scores.get(word, 0)
            tfidf[word] = tf * idf
        
        self._word_vectors[note.name] = tfidf
        return tfidf
    
    def _cosine_similarity(self, vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2:
            return 0.0
        
        # è®¡ç®—ç‚¹ç§¯
        common_words = set(vec1.keys()) & set(vec2.keys())
        dot_product = sum(vec1[w] * vec2[w] for w in common_words)
        
        # è®¡ç®—å‘é‡é•¿åº¦
        norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
        norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„è¯é‡å ï¼‰"""
        words1 = set(self._tokenize(title1))
        words2 = set(self._tokenize(title2))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _tag_similarity(self, tags1: Set[str], tags2: Set[str]) -> float:
        """æ ‡ç­¾ç›¸ä¼¼åº¦"""
        if not tags1 or not tags2:
            return 0.0
        
        intersection = len(tags1 & tags2)
        union = len(tags1 | tags2)
        
        return intersection / union if union > 0 else 0.0
    
    def find_similar_notes(self, note_name: str, top_n: int = 5) -> List[SimilarityResult]:
        """æ‰¾å‡ºä¸æŒ‡å®šç¬”è®°ç›¸ä¼¼çš„ç¬”è®°"""
        if note_name not in self.notes:
            return []
        
        target_note = self.notes[note_name]
        target_vec = self._get_tfidf_vector(target_note)
        
        results = []
        
        for other_name, other_note in self.notes.items():
            if other_name == note_name:
                continue
            
            # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
            other_vec = self._get_tfidf_vector(other_note)
            content_sim = self._cosine_similarity(target_vec, other_vec)
            
            # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
            title_sim = self._title_similarity(target_note.name, other_note.name)
            
            # è®¡ç®—æ ‡ç­¾ç›¸ä¼¼åº¦
            tag_sim = self._tag_similarity(target_note.tags, other_note.tags)
            
            # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒï¼‰
            total_sim = content_sim * 0.6 + title_sim * 0.2 + tag_sim * 0.2
            
            if total_sim >= self.min_similarity:
                # æ‰¾å‡ºå…±åŒçš„å…³é”®è¯
                common = set(target_vec.keys()) & set(other_vec.keys())
                common_words = sorted(common, 
                                     key=lambda w: target_vec[w] + other_vec[w], 
                                     reverse=True)[:10]
                
                # åˆ¤æ–­ä¸»è¦ç›¸ä¼¼åŸå› 
                if content_sim > 0.5:
                    reason = 'content'
                elif title_sim > 0.3:
                    reason = 'title'
                elif tag_sim > 0.3:
                    reason = 'tags'
                else:
                    reason = 'mixed'
                
                results.append(SimilarityResult(
                    note1=note_name,
                    note2=other_name,
                    similarity=total_sim,
                    common_words=common_words,
                    reason=reason
                ))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.similarity, reverse=True)
        
        return results[:top_n]
    
    def find_all_similar_pairs(self, min_similarity: float = None) -> List[SimilarityResult]:
        """æ‰¾å‡ºæ‰€æœ‰ç›¸ä¼¼çš„ç¬”è®°å¯¹"""
        if min_similarity is None:
            min_similarity = self.min_similarity
        
        print("ğŸ”„ Computing TF-IDF vectors...")
        # é¢„è®¡ç®— IDF
        self._calculate_idf()
        
        # é¢„è®¡ç®—æ‰€æœ‰ç¬”è®°çš„å‘é‡
        for note in self.notes.values():
            self._get_tfidf_vector(note)
        
        results = []
        note_names = list(self.notes.keys())
        total_pairs = len(note_names) * (len(note_names) - 1) // 2
        
        print(f"ğŸ” Analyzing {total_pairs} note pairs...")
        
        processed = 0
        for i, name1 in enumerate(note_names):
            note1 = self.notes[name1]
            vec1 = self._word_vectors[name1]
            
            for name2 in note_names[i + 1:]:
                note2 = self.notes[name2]
                vec2 = self._word_vectors[name2]
                
                # è®¡ç®—å„ç»´åº¦ç›¸ä¼¼åº¦
                content_sim = self._cosine_similarity(vec1, vec2)
                title_sim = self._title_similarity(name1, name2)
                tag_sim = self._tag_similarity(note1.tags, note2.tags)
                
                # ç»¼åˆç›¸ä¼¼åº¦
                total_sim = content_sim * 0.6 + title_sim * 0.2 + tag_sim * 0.2
                
                if total_sim >= min_similarity:
                    common = set(vec1.keys()) & set(vec2.keys())
                    common_words = sorted(common,
                                        key=lambda w: vec1[w] + vec2[w],
                                        reverse=True)[:10]
                    
                    if content_sim > 0.5:
                        reason = 'content'
                    elif title_sim > 0.3:
                        reason = 'title'
                    elif tag_sim > 0.3:
                        reason = 'tags'
                    else:
                        reason = 'mixed'
                    
                    results.append(SimilarityResult(
                        note1=name1,
                        note2=name2,
                        similarity=total_sim,
                        common_words=common_words,
                        reason=reason
                    ))
                
                processed += 1
                if processed % 1000 == 0:
                    print(f"  Processed {processed}/{total_pairs} pairs...")
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.similarity, reverse=True)
        
        print(f"âœ… Found {len(results)} similar pairs")
        
        return results
    
    def find_potential_duplicates(self, threshold: float = 0.7) -> List[SimilarityResult]:
        """æ‰¾å‡ºå¯èƒ½é‡å¤çš„ç¬”è®°ï¼ˆé«˜ç›¸ä¼¼åº¦ï¼‰"""
        all_similar = self.find_all_similar_pairs(min_similarity=threshold)
        
        # è¿‡æ»¤å‡ºé«˜ç›¸ä¼¼åº¦çš„
        duplicates = [s for s in all_similar if s.similarity >= threshold]
        
        return duplicates
    
    def find_related_unlinked(self) -> List[Tuple[SimilarityResult, bool]]:
        """æ‰¾å‡ºç›¸å…³ä½†æœªé“¾æ¥çš„ç¬”è®°"""
        all_similar = self.find_all_similar_pairs()
        
        results = []
        for sim in all_similar:
            note1 = self.notes[sim.note1]
            note2 = self.notes[sim.note2]
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰é“¾æ¥
            has_link = (sim.note2 in note1.outgoing_links or 
                       sim.note1 in note2.outgoing_links)
            
            if not has_link:
                results.append((sim, False))
            else:
                results.append((sim, True))
        
        return results
    
    def suggest_merges(self, min_similarity: float = 0.6) -> List[Tuple[str, str, float, List[str]]]:
        """å»ºè®®åˆå¹¶çš„ç¬”è®°"""
        duplicates = self.find_potential_duplicates(threshold=min_similarity)
        
        suggestions = []
        for dup in duplicates:
            note1 = self.notes[dup.note1]
            note2 = self.notes[dup.note2]
            
            # å¦‚æœä¸¤ä¸ªç¬”è®°éƒ½å¾ˆçŸ­ï¼Œæ›´å¯èƒ½éœ€è¦åˆå¹¶
            if note1.word_count < 200 and note2.word_count < 200:
                reasons = ["ä¸¤ä¸ªç¬”è®°éƒ½å¾ˆçŸ­", f"å†…å®¹ç›¸ä¼¼åº¦: {dup.similarity:.1%}"]
                suggestions.append((dup.note1, dup.note2, dup.similarity, reasons))
            elif dup.similarity > 0.8:
                reasons = [f"å†…å®¹é«˜åº¦ç›¸ä¼¼: {dup.similarity:.1%}"]
                if dup.reason == 'title':
                    reasons.append("æ ‡é¢˜ç›¸ä¼¼")
                suggestions.append((dup.note1, dup.note2, dup.similarity, reasons))
        
        return suggestions
    
    def get_statistics(self, results: List[SimilarityResult]) -> Dict:
        """è·å–ç›¸ä¼¼åº¦ç»Ÿè®¡"""
        if not results:
            return {
                'total_pairs': 0,
                'high_similarity': 0,
                'medium_similarity': 0,
                'low_similarity': 0,
                'avg_similarity': 0
            }
        
        similarities = [r.similarity for r in results]
        
        return {
            'total_pairs': len(results),
            'high_similarity': len([s for s in similarities if s >= 0.7]),
            'medium_similarity': len([s for s in similarities if 0.5 <= s < 0.7]),
            'low_similarity': len([s for s in similarities if s < 0.5]),
            'avg_similarity': sum(similarities) / len(similarities),
            'max_similarity': max(similarities),
            'min_similarity': min(similarities)
        }


if __name__ == '__main__':
    print("This module should be imported, not run directly.")
