#!/usr/bin/env python3
"""
Obsidian Knowledge Assistant - Core Analyzer
åˆ†æ Obsidian vault çš„ç¬”è®°ç»“æ„å’Œè¿æ¥å…³ç³»
"""

import os
import re
from pathlib import Path
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import List, Dict, Set, Tuple


@dataclass
class Note:
    """ç¬”è®°æ•°æ®ç»“æ„"""

    path: Path
    name: str
    content: str
    word_count: int
    outgoing_links: Set[str]  # è¯¥ç¬”è®°é“¾æ¥åˆ°çš„å…¶ä»–ç¬”è®°
    incoming_links: Set[str]  # é“¾æ¥åˆ°è¯¥ç¬”è®°çš„å…¶ä»–ç¬”è®°
    tags: Set[str]
    created_time: datetime
    modified_time: datetime

    @property
    def is_orphan(self) -> bool:
        """æ˜¯å¦ä¸ºå­¤å²›ç¬”è®°ï¼ˆæ²¡æœ‰ä»»ä½•é“¾æ¥å…³ç³»ï¼‰"""
        return len(self.outgoing_links) == 0 and len(self.incoming_links) == 0

    @property
    def total_links(self) -> int:
        """æ€»é“¾æ¥æ•°"""
        return len(self.outgoing_links) + len(self.incoming_links)


class ObsidianAnalyzer:
    """Obsidian vault åˆ†æå™¨"""

    def __init__(
        self,
        vault_path: str,
        exclude_folders: List[str] = None,
        exclude_notes: List[str] = None,
    ):
        self.vault_path = Path(vault_path)
        self.exclude_folders = exclude_folders or [".obsidian", ".trash"]
        self.exclude_notes = exclude_notes or []
        self.notes: Dict[str, Note] = {}
        self.link_pattern = re.compile(r"\[\[([^\]]+)\]\]")
        self.tag_pattern = re.compile(r"#([\w\-/]+)")

    def _should_exclude_note(self, note_name: str) -> bool:
        """æ£€æŸ¥ç¬”è®°æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        for pattern in self.exclude_notes:
            # ç®€å•çš„é€šé…ç¬¦æ”¯æŒ
            if "*" in pattern:
                import fnmatch

                if fnmatch.fnmatch(note_name, pattern):
                    return True
            elif pattern == note_name:
                return True
        return False

    def scan_vault(self) -> None:
        """æ‰«ææ•´ä¸ª vault"""
        print(f"ğŸ” Scanning vault: {self.vault_path}")

        md_files = []
        for md_file in self.vault_path.rglob("*.md"):
            # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®å½•ä¸­
            if any(excluded in md_file.parts for excluded in self.exclude_folders):
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ’é™¤çš„ç¬”è®°
            note_name = md_file.stem
            if self._should_exclude_note(note_name):
                continue

            md_files.append(md_file)

        print(f"ğŸ“ Found {len(md_files)} markdown files")

        # ç¬¬ä¸€éï¼šè§£ææ‰€æœ‰ç¬”è®°
        for md_file in md_files:
            self._parse_note(md_file)

        # ç¬¬äºŒéï¼šå»ºç«‹åå‘é“¾æ¥ï¼ˆincoming linksï¼‰
        self._build_incoming_links()

        print(f"âœ… Analysis complete: {len(self.notes)} notes processed")

    def _parse_note(self, file_path: Path) -> None:
        """è§£æå•ä¸ªç¬”è®°"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # è·å–ç¬”è®°åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
            note_name = file_path.stem

            # æå–é“¾æ¥
            outgoing_links = set()
            for match in self.link_pattern.finditer(content):
                link = match.group(1)
                # å¤„ç†åˆ«å [[note|alias]]
                if "|" in link:
                    link = link.split("|")[0]
                # å¤„ç†æ ‡é¢˜é“¾æ¥ [[note#heading]]
                if "#" in link:
                    link = link.split("#")[0]
                outgoing_links.add(link.strip())

            # æå–æ ‡ç­¾
            tags = set(self.tag_pattern.findall(content))

            # ç»Ÿè®¡å­—æ•°ï¼ˆç®€å•ç»Ÿè®¡ï¼Œæ’é™¤ä»£ç å—ï¼‰
            text_content = re.sub(r"```.*?```", "", content, flags=re.DOTALL)
            word_count = len(text_content.split())

            # è·å–æ–‡ä»¶æ—¶é—´
            stat = file_path.stat()
            created_time = datetime.fromtimestamp(stat.st_ctime)
            modified_time = datetime.fromtimestamp(stat.st_mtime)

            # åˆ›å»º Note å¯¹è±¡
            note = Note(
                path=file_path,
                name=note_name,
                content=content,
                word_count=word_count,
                outgoing_links=outgoing_links,
                incoming_links=set(),  # ç¨åå¡«å……
                tags=tags,
                created_time=created_time,
                modified_time=modified_time,
            )

            self.notes[note_name] = note

        except Exception as e:
            print(f"âš ï¸  Error parsing {file_path}: {e}")

    def _build_incoming_links(self) -> None:
        """å»ºç«‹åå‘é“¾æ¥å…³ç³»"""
        for note_name, note in self.notes.items():
            for linked_note in note.outgoing_links:
                if linked_note in self.notes:
                    self.notes[linked_note].incoming_links.add(note_name)

    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        total_notes = len(self.notes)
        total_words = sum(note.word_count for note in self.notes.values())

        # å­¤å²›ç¬”è®°
        orphan_notes = [note for note in self.notes.values() if note.is_orphan]
        orphan_notes.sort(key=lambda x: x.modified_time, reverse=True)

        # é“¾æ¥æœ€å¤šçš„ç¬”è®°ï¼ˆå‡ºé“¾ï¼‰
        most_outgoing = sorted(
            self.notes.values(), key=lambda x: len(x.outgoing_links), reverse=True
        )

        # è¢«é“¾æ¥æœ€å¤šçš„ç¬”è®°ï¼ˆå…¥é“¾ï¼‰
        most_incoming = sorted(
            self.notes.values(), key=lambda x: len(x.incoming_links), reverse=True
        )

        # æ ‡ç­¾ç»Ÿè®¡
        all_tags = []
        for note in self.notes.values():
            all_tags.extend(note.tags)
        tag_counter = Counter(all_tags)

        # æ— æ ‡ç­¾ç¬”è®°
        untagged_notes = [note for note in self.notes.values() if len(note.tags) == 0]

        # æ—¶é—´åˆ†å¸ƒ
        now = datetime.now()
        recent_counts = {"7_days": 0, "30_days": 0, "90_days": 0}

        for note in self.notes.values():
            days_ago = (now - note.modified_time).days
            if days_ago <= 7:
                recent_counts["7_days"] += 1
            if days_ago <= 30:
                recent_counts["30_days"] += 1
            if days_ago <= 90:
                recent_counts["90_days"] += 1

        # é“¾æ¥ç»Ÿè®¡
        total_links = sum(len(note.outgoing_links) for note in self.notes.values())
        bidirectional_links = (
            sum(
                1
                for note in self.notes.values()
                for link in note.outgoing_links
                if link in self.notes and note.name in self.notes[link].outgoing_links
            )
            // 2
        )  # é™¤ä»¥2å› ä¸ºåŒå‘é“¾æ¥è¢«è®¡ç®—äº†ä¸¤æ¬¡

        return {
            "total_notes": total_notes,
            "total_words": total_words,
            "orphan_notes": orphan_notes,
            "most_outgoing": most_outgoing,
            "most_incoming": most_incoming,
            "tag_counter": tag_counter,
            "untagged_notes": untagged_notes,
            "recent_counts": recent_counts,
            "total_links": total_links,
            "bidirectional_links": bidirectional_links,
            "avg_word_count": total_words // total_notes if total_notes > 0 else 0,
            "avg_links_per_note": total_links / total_notes if total_notes > 0 else 0,
        }

    def search_notes(
        self,
        query: str = None,
        tags: List[str] = None,
        min_links: int = None,
        max_links: int = None,
    ) -> List[Note]:
        """æœç´¢ç¬”è®°

        Args:
            query: å…³é”®è¯æœç´¢ï¼ˆåœ¨ç¬”è®°åç§°å’Œå†…å®¹ä¸­æœç´¢ï¼‰
            tags: æ ‡ç­¾åˆ—è¡¨ï¼ˆç¬”è®°å¿…é¡»åŒ…å«æ‰€æœ‰è¿™äº›æ ‡ç­¾ï¼‰
            min_links: æœ€å°é“¾æ¥æ•°
            max_links: æœ€å¤§é“¾æ¥æ•°

        Returns:
            ç¬¦åˆæ¡ä»¶çš„ç¬”è®°åˆ—è¡¨
        """
        results = list(self.notes.values())

        # å…³é”®è¯è¿‡æ»¤
        if query:
            query_lower = query.lower()
            results = [
                note
                for note in results
                if query_lower in note.name.lower()
                or query_lower in note.content.lower()
            ]

        # æ ‡ç­¾è¿‡æ»¤
        if tags:
            tag_set = set(tags)
            results = [note for note in results if tag_set.issubset(note.tags)]

        # é“¾æ¥æ•°è¿‡æ»¤
        if min_links is not None:
            results = [note for note in results if note.total_links >= min_links]

        if max_links is not None:
            results = [note for note in results if note.total_links <= max_links]

        return results


class MultiVaultAnalyzer:
    """å¤š vault åˆ†æå™¨"""

    def __init__(
        self,
        vault_paths: List[str],
        exclude_folders: List[str] = None,
        exclude_notes: List[str] = None,
    ):
        self.vault_paths = [Path(p) for p in vault_paths]
        self.exclude_folders = exclude_folders
        self.exclude_notes = exclude_notes
        self.analyzers: Dict[str, ObsidianAnalyzer] = {}
        self.combined_stats = None

    def scan_all_vaults(self) -> None:
        """æ‰«ææ‰€æœ‰ vaults"""
        print(f"ğŸ” Scanning {len(self.vault_paths)} vaults...")
        print()

        for vault_path in self.vault_paths:
            vault_name = vault_path.name
            print(f"ğŸ“‚ Vault: {vault_name}")

            analyzer = ObsidianAnalyzer(
                str(vault_path), self.exclude_folders, self.exclude_notes
            )
            analyzer.scan_vault()
            self.analyzers[vault_name] = analyzer
            print()

    def get_combined_statistics(self) -> Dict:
        """è·å–æ‰€æœ‰ vaults çš„åˆå¹¶ç»Ÿè®¡"""
        if not self.analyzers:
            return {}

        total_notes = sum(len(a.notes) for a in self.analyzers.values())
        total_words = sum(
            sum(n.word_count for n in a.notes.values()) for a in self.analyzers.values()
        )

        all_orphans = []
        all_tags = []
        total_links = 0

        for analyzer in self.analyzers.values():
            stats = analyzer.get_statistics()
            all_orphans.extend(stats["orphan_notes"])
            total_links += stats["total_links"]

            for note in analyzer.notes.values():
                all_tags.extend(note.tags)

        tag_counter = Counter(all_tags)

        return {
            "total_vaults": len(self.analyzers),
            "total_notes": total_notes,
            "total_words": total_words,
            "total_orphans": len(all_orphans),
            "total_links": total_links,
            "total_unique_tags": len(tag_counter),
            "vault_breakdown": {
                name: {
                    "notes": len(a.notes),
                    "words": sum(n.word_count for n in a.notes.values()),
                    "orphans": len([n for n in a.notes.values() if n.is_orphan]),
                }
                for name, a in self.analyzers.items()
            },
        }

    def search_across_vaults(self, **search_params) -> Dict[str, List[Note]]:
        """åœ¨æ‰€æœ‰ vaults ä¸­æœç´¢"""
        results = {}
        for vault_name, analyzer in self.analyzers.items():
            vault_results = analyzer.search_notes(**search_params)
            if vault_results:
                results[vault_name] = vault_results
        return results


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    vault_path = os.getenv("VAULT_PATH", "F:/Project/Obsidian")
    exclude_folders = os.getenv("EXCLUDE_FOLDERS", ".obsidian,.trash").split(",")
    exclude_notes = (
        os.getenv("EXCLUDE_NOTES", "").split(",") if os.getenv("EXCLUDE_NOTES") else []
    )

    analyzer = ObsidianAnalyzer(vault_path, exclude_folders, exclude_notes)
    analyzer.scan_vault()

    stats = analyzer.get_statistics()
    print(f"\nğŸ“Š Statistics:")
    print(f"   Total notes: {stats['total_notes']}")
    print(f"   Total words: {stats['total_words']:,}")
    print(f"   Orphan notes: {len(stats['orphan_notes'])}")
    print(f"   Total links: {stats['total_links']}")
