#!/usr/bin/env python3
"""
Obsidian Knowledge Assistant - Similarity Analysis Tool
ç›¸ä¼¼åº¦åˆ†æå‘½ä»¤è¡Œå·¥å…·
"""

import os
import sys
import argparse
from pathlib import Path
import sys

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.analyzer import ObsidianAnalyzer
from core.similarity import SimilarityAnalyzer


def find_similar_command(analyzer: SimilarityAnalyzer, args):
    """æŸ¥æ‰¾ä¸æŒ‡å®šç¬”è®°ç›¸ä¼¼çš„ç¬”è®°"""
    results = analyzer.find_similar_notes(args.note_name, top_n=args.limit)

    if not results:
        print(f"\nâŒ æœªæ‰¾åˆ°ä¸ '{args.note_name}' ç›¸ä¼¼çš„ç¬”è®°")
        print("   æç¤º: å¯èƒ½ç¬”è®°åç§°ä¸æ­£ç¡®ï¼Œæˆ–ç›¸ä¼¼åº¦é˜ˆå€¼å¤ªé«˜")
        return

    print(f"\n{'='*60}")
    print(f"  ğŸ“ ä¸ '{args.note_name}' ç›¸ä¼¼çš„ç¬”è®°")
    print(f"{'='*60}")
    print()

    for i, result in enumerate(results, 1):
        print(f"{i}. {result.note2}")
        print(f"   ç›¸ä¼¼åº¦: {result.similarity:.1%} | åŸå› : {result.reason}")
        if result.common_words:
            print(f"   å…±åŒå…³é”®è¯: {', '.join(result.common_words[:5])}")
        print()


def find_duplicates_command(analyzer: SimilarityAnalyzer, args):
    """æŸ¥æ‰¾å¯èƒ½é‡å¤çš„ç¬”è®°"""
    threshold = args.threshold if hasattr(args, "threshold") and args.threshold else 0.7

    duplicates = analyzer.find_potential_duplicates(threshold=threshold)

    if not duplicates:
        print(f"\nâœ… æœªå‘ç°ç›¸ä¼¼åº¦ â‰¥{threshold:.0%} çš„é‡å¤ç¬”è®°")
        return

    print(f"\n{'='*60}")
    print(f"  âš ï¸  å¯èƒ½é‡å¤çš„ç¬”è®° (ç›¸ä¼¼åº¦ â‰¥{threshold:.0%})")
    print(f"{'='*60}")
    print()

    for i, dup in enumerate(duplicates[: args.limit], 1):
        print(f"{i}. {dup.note1} â†â†’ {dup.note2}")
        print(f"   ç›¸ä¼¼åº¦: {dup.similarity:.1%}")
        print(f"   åŸå› : {dup.reason}")
        if dup.common_words:
            print(f"   å…±åŒè¯: {', '.join(dup.common_words[:5])}")
        print()


def find_unlinked_command(analyzer: SimilarityAnalyzer, args):
    """æŸ¥æ‰¾ç›¸å…³ä½†æœªé“¾æ¥çš„ç¬”è®°"""
    print("\nğŸ” åˆ†æç›¸å…³ç¬”è®°çš„é“¾æ¥çŠ¶æ€...")

    related = analyzer.find_related_unlinked()

    # åªæ˜¾ç¤ºæœªé“¾æ¥çš„
    unlinked = [(sim, linked) for sim, linked in related if not linked]

    if not unlinked:
        print("\nâœ… æ‰€æœ‰ç›¸å…³ç¬”è®°éƒ½å·²å»ºç«‹é“¾æ¥ï¼")
        return

    print(f"\n{'='*60}")
    print(f"  ğŸ”— ç›¸å…³ä½†æœªé“¾æ¥çš„ç¬”è®° (å»ºè®®æ·»åŠ é“¾æ¥)")
    print(f"{'='*60}")
    print()

    for i, (sim, _) in enumerate(unlinked[: args.limit], 1):
        print(f"{i}. {sim.note1} â†â†’ {sim.note2}")
        print(f"   ç›¸ä¼¼åº¦: {sim.similarity:.1%}")
        print(f"   ğŸ’¡ å»ºè®®: è€ƒè™‘åœ¨ç¬”è®°é—´æ·»åŠ  [[é“¾æ¥]]")
        if sim.common_words:
            print(f"   å…±åŒä¸»é¢˜: {', '.join(sim.common_words[:5])}")
        print()


def suggest_merges_command(analyzer: SimilarityAnalyzer, args):
    """å»ºè®®åˆå¹¶çš„ç¬”è®°"""
    print("\nğŸ” åˆ†æå¯åˆå¹¶çš„ç¬”è®°...")

    threshold = args.threshold if hasattr(args, "threshold") and args.threshold else 0.6
    suggestions = analyzer.suggest_merges(min_similarity=threshold)

    if not suggestions:
        print(f"\nâœ… æœªå‘ç°éœ€è¦åˆå¹¶çš„ç¬”è®°")
        return

    print(f"\n{'='*60}")
    print(f"  ğŸ“¦ å»ºè®®åˆå¹¶çš„ç¬”è®°")
    print(f"{'='*60}")
    print()

    for i, (note1, note2, similarity, reasons) in enumerate(
        suggestions[: args.limit], 1
    ):
        print(f"{i}. {note1} + {note2}")
        print(f"   ç›¸ä¼¼åº¦: {similarity:.1%}")
        print(f"   åŸå› :")
        for reason in reasons:
            print(f"     â€¢ {reason}")
        print()


def analyze_all_command(analyzer: SimilarityAnalyzer, args):
    """åˆ†ææ‰€æœ‰ç›¸ä¼¼ç¬”è®°å¯¹"""
    threshold = args.threshold if hasattr(args, "threshold") and args.threshold else 0.3

    results = analyzer.find_all_similar_pairs(min_similarity=threshold)

    if not results:
        print(f"\næœªå‘ç°ç›¸ä¼¼åº¦ â‰¥{threshold:.0%} çš„ç¬”è®°å¯¹")
        return

    # æ˜¾ç¤ºç»Ÿè®¡
    stats = analyzer.get_statistics(results)

    print(f"\n{'='*60}")
    print(f"  ğŸ“Š ç›¸ä¼¼åº¦åˆ†æç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"  ç›¸ä¼¼ç¬”è®°å¯¹:   {stats['total_pairs']}")
    print(f"  é«˜ç›¸ä¼¼(â‰¥70%): {stats['high_similarity']}")
    print(f"  ä¸­ç­‰(50-70%): {stats['medium_similarity']}")
    print(f"  ä½ç›¸ä¼¼(<50%): {stats['low_similarity']}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦:   {stats['avg_similarity']:.1%}")
    print(f"  æœ€é«˜ç›¸ä¼¼åº¦:   {stats['max_similarity']:.1%}")
    print(f"{'='*60}")
    print()

    # æ˜¾ç¤º Top ç»“æœ
    print(f"Top {min(args.limit, len(results))} æœ€ç›¸ä¼¼çš„ç¬”è®°å¯¹:")
    print()

    for i, result in enumerate(results[: args.limit], 1):
        print(f"{i}. {result.note1} â†â†’ {result.note2}")
        print(f"   ç›¸ä¼¼åº¦: {result.similarity:.1%} | åŸå› : {result.reason}")
        if result.common_words:
            print(f"   å…±åŒè¯: {', '.join(result.common_words[:5])}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Obsidian Knowledge Assistant - Similarity Analysis Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--vault", type=str, help="Vault path (overrides VAULT_PATH env var)"
    )

    subparsers = parser.add_subparsers(dest="command", help="Commands")

    # find å‘½ä»¤ - æŸ¥æ‰¾ä¸æŒ‡å®šç¬”è®°ç›¸ä¼¼çš„ç¬”è®°
    find_parser = subparsers.add_parser(
        "find", help="Find notes similar to a specific note"
    )
    find_parser.add_argument("note_name", help="Note name to find similar notes for")
    find_parser.add_argument(
        "--limit", type=int, default=5, help="Number of results (default: 5)"
    )

    # duplicates å‘½ä»¤ - æŸ¥æ‰¾å¯èƒ½é‡å¤çš„ç¬”è®°
    dup_parser = subparsers.add_parser(
        "duplicates", help="Find potential duplicate notes"
    )
    dup_parser.add_argument(
        "--threshold", type=float, help="Similarity threshold (default: 0.7)"
    )
    dup_parser.add_argument(
        "--limit", type=int, default=20, help="Number of results (default: 20)"
    )

    # unlinked å‘½ä»¤ - æŸ¥æ‰¾ç›¸å…³ä½†æœªé“¾æ¥çš„ç¬”è®°
    unlink_parser = subparsers.add_parser(
        "unlinked", help="Find related but unlinked notes"
    )
    unlink_parser.add_argument(
        "--limit", type=int, default=20, help="Number of results (default: 20)"
    )

    # merge å‘½ä»¤ - å»ºè®®åˆå¹¶çš„ç¬”è®°
    merge_parser = subparsers.add_parser("merge", help="Suggest notes to merge")
    merge_parser.add_argument(
        "--threshold", type=float, help="Similarity threshold (default: 0.6)"
    )
    merge_parser.add_argument(
        "--limit", type=int, default=10, help="Number of results (default: 10)"
    )

    # all å‘½ä»¤ - åˆ†ææ‰€æœ‰ç›¸ä¼¼ç¬”è®°å¯¹
    all_parser = subparsers.add_parser("all", help="Analyze all similar note pairs")
    all_parser.add_argument(
        "--threshold", type=float, help="Minimum similarity threshold (default: 0.3)"
    )
    all_parser.add_argument(
        "--limit",
        type=int,
        default=20,
        help="Number of results to display (default: 20)",
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # è·å– vault è·¯å¾„
    vault_path = args.vault or os.getenv("VAULT_PATH")
    if not vault_path:
        print("âŒ Error: Vault path not specified")
        print("   Use --vault or set VAULT_PATH environment variable")
        sys.exit(1)

    vault_path = Path(vault_path)
    if not vault_path.exists():
        print(f"âŒ Error: Vault does not exist: {vault_path}")
        sys.exit(1)

    # åŠ è½½é…ç½®
    exclude_folders = os.getenv("EXCLUDE_FOLDERS", ".obsidian,.trash").split(",")
    exclude_notes = (
        os.getenv("EXCLUDE_NOTES", "").split(",") if os.getenv("EXCLUDE_NOTES") else []
    )

    # åˆ›å»ºåˆ†æå™¨
    print(f"ğŸ” Loading vault: {vault_path}")
    obs_analyzer = ObsidianAnalyzer(str(vault_path), exclude_folders, exclude_notes)
    obs_analyzer.scan_vault()

    print("ğŸ§® Initializing similarity analyzer...")
    sim_analyzer = SimilarityAnalyzer(obs_analyzer.notes)

    # æ‰§è¡Œå‘½ä»¤
    try:
        if args.command == "find":
            find_similar_command(sim_analyzer, args)

        elif args.command == "duplicates":
            find_duplicates_command(sim_analyzer, args)

        elif args.command == "unlinked":
            find_unlinked_command(sim_analyzer, args)

        elif args.command == "merge":
            suggest_merges_command(sim_analyzer, args)

        elif args.command == "all":
            analyze_all_command(sim_analyzer, args)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Analysis interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
